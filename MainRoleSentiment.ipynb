{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EntityDataset import TrainDataset\n",
    "from LoadData import LoadData\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LoadData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m base_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m txt_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubtask-1-annotations.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 14\u001b[0m ld \u001b[38;5;241m=\u001b[39m \u001b[43mLoadData\u001b[49m()\n\u001b[1;32m     16\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLanguage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision_Protagonist\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision_Antagonist\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision_Innocent\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecall_Protagonist\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecall_Antagonist\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecall_Innocent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lang_idx, lang \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(languages):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LoadData' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "sentiments = [\"Very Negative\", \"Negative\", \"Neutral\", \"Positive\", \"Very Positive\"]\n",
    "main_roles = [\"Protagonist\", \"Antagonist\", \"Innocent\"]\n",
    "sentiment_to_main_role = {\n",
    "    \"Very Negative\": \"Antagonist\",\n",
    "    \"Negative\": \"Antagonist\",\n",
    "    \"Neutral\": \"Innocent\",\n",
    "    \"Positive\": \"Protagonist\",\n",
    "    \"Very Positive\": \"Protagonist\"\n",
    "}\n",
    "languages = [\"EN\", \"RU\", \"PT\", \"HI\", \"BG\"]\n",
    "base_dir = \"train\"\n",
    "txt_file = \"subtask-1-annotations.txt\"\n",
    "\n",
    "ld = LoadData()\n",
    "\n",
    "results_df = pd.DataFrame(columns = ['Language', 'Accuracy', 'Precision_Protagonist', 'Precision_Antagonist', 'Precision_Innocent', 'Recall_Protagonist', 'Recall_Antagonist', 'Recall_Innocent'])\n",
    "\n",
    "for lang_idx, lang in enumerate(languages):\n",
    "\n",
    "    total_instances = 0\n",
    "    match = 0\n",
    "    # Load data for each language\n",
    "    data = ld.load_data(base_dir, txt_file, lang)\n",
    "    train_dataset = TrainDataset(data, base_dir, language=lang, return_sentiment=True, coref=False)\n",
    "    \n",
    "    df = pd.DataFrame(columns=['article_id', 'entity_mention', 'main_role', 'predicted_main_role'])\n",
    "\n",
    "    for idx, sample in enumerate(train_dataset):\n",
    "\n",
    "        index = np.argmax(sample['sent_sent'])\n",
    "\n",
    "        sent_main_role = sentiment_to_main_role[sentiments[index]]\n",
    "        true_main_role = sample[\"main_role\"]\n",
    "\n",
    "        if sent_main_role == true_main_role:\n",
    "            match += 1\n",
    "        \n",
    "        total_instances += 1\n",
    "\n",
    "        df = pd.concat([df, pd.DataFrame([{\n",
    "            'article_id': sample['article_id'],\n",
    "            'entity_mention': sample['entity_mention'],\n",
    "            'main_role': true_main_role,\n",
    "            'predicted_main_role': sent_main_role\n",
    "        }])], ignore_index=True)\n",
    "        \n",
    "    accuracy = match / total_instances\n",
    "\n",
    "    # Calculate precision and recall for each main role\n",
    "    precision_dict = {}\n",
    "    recall_dict = {}\n",
    "\n",
    "    for role in main_roles:\n",
    "        true_positives = len(df[(df['main_role'] == role) & (df['predicted_main_role'] == role)])\n",
    "        predicted_positives = len(df[df['predicted_main_role'] == role])\n",
    "        actual_positives = len(df[df['main_role'] == role])\n",
    "        \n",
    "        precision = true_positives / predicted_positives if predicted_positives > 0 else 0\n",
    "        recall = true_positives / actual_positives if actual_positives > 0 else 0\n",
    "        \n",
    "        precision_dict[role] = precision\n",
    "        recall_dict[role] = recall\n",
    "\n",
    "    # Create a row for the CSV\n",
    "    results_df = pd.concat([results_df,pd.DataFrame({\n",
    "        'Language': [lang],\n",
    "        'Accuracy': [accuracy],\n",
    "        **{f'Precision_{role}': [precision_dict[role]] for role in main_roles},\n",
    "        **{f'Recall_{role}': [recall_dict[role]] for role in main_roles}\n",
    "        \n",
    "    })], ignore_index= True)\n",
    "    \n",
    "    # Append to CSV file, create if doesn't exist\n",
    "    \n",
    "\n",
    "    df.to_csv(f\"./sentiment_res/{lang}_sentiment.csv\", index=False)\n",
    "results_df.to_csv('./sentiment_res/evaluation.csv', header=not pd.io.common.file_exists('./sentiment_res/sentiment.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Parsing Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"></br>\n",
       "<mark class=\"entity\" style=\"background: lightblue; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    NASA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">↠ launched</span>\n",
       "</mark>\n",
       " launched a new satellite into orbit. The satellite will observe solar storms. </br>\n",
       "<mark class=\"entity\" style=\"background: lightblue; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Elon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">↠ announced</span>\n",
       "</mark>\n",
       " Musk announced a new partnership with \n",
       "<mark class=\"entity\" style=\"background: lightblue; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    NASA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">↠ partnership</span>\n",
       "</mark>\n",
       ". </br>The mission is expected to last two years.</br></div>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'ent' visualizer\n",
      "Serving on http://0.0.0.0:5001 ...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [11/Mar/2025 16:49:20] \"GET / HTTP/1.1\" 200 1553\n",
      "127.0.0.1 - - [11/Mar/2025 16:49:20] \"GET /favicon.ico HTTP/1.1\" 200 1553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down server on port 5001.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text (you can replace this with reading from a file)\n",
    "text = \"\"\"\n",
    "NASA launched a new satellite into orbit. The satellite will observe solar storms. \n",
    "Elon Musk announced a new partnership with NASA. \n",
    "The mission is expected to last two years.\n",
    "\"\"\"\n",
    "\n",
    "# Process the document\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract named entities we care about\n",
    "entities = {ent.text for ent in doc.ents if ent.label_ in {\"PERSON\", \"ORG\", \"GPE\"}}\n",
    "\n",
    "# Recursive function to trace up the dependency tree until a verb or meaningful head is found\n",
    "def trace_dependency_path(token, depth=0):\n",
    "    if token.pos_ == \"VERB\" or token.dep_ in {\"attr\", \"acomp\", \"dobj\"}:\n",
    "        return token\n",
    "    if token.head == token or depth > 5:  # prevent infinite loop\n",
    "        return token\n",
    "    return trace_dependency_path(token.head, depth + 1)\n",
    "\n",
    "# Collect (entity_token, defining_token) pairs\n",
    "arrows = []\n",
    "for token in doc:\n",
    "    for ent in entities:\n",
    "        if token.text == ent.split()[0]:  # basic match on first word of entity\n",
    "            root = trace_dependency_path(token)\n",
    "            if root != token:\n",
    "                arrows.append((token, root))\n",
    "\n",
    "# Convert to custom spans for visualization\n",
    "custom_spans = []\n",
    "for start, end in arrows:\n",
    "    label = f\"↠ {end.text}\"\n",
    "    span = Span(doc, start.i, start.i + 1, label=label)\n",
    "    custom_spans.append(span)\n",
    "\n",
    "# Replace the document's entities with our custom ones\n",
    "doc.set_ents([], default=\"unmodified\")\n",
    "doc.ents = custom_spans\n",
    "\n",
    "# Generate unique colors for each arrow label\n",
    "options = {\n",
    "    \"colors\": {span.label_: \"lightblue\" for span in custom_spans}\n",
    "}\n",
    "\n",
    "# Launch visualization in browser\n",
    "displacy.serve(doc, style=\"ent\", options=options, auto_select_port=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"NASA\": [['Object', 'with NASA']]\n",
      "\"Elon Musk\": [['Subject', 'announced a new partnership with NASA']]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample article\n",
    "text = \"\"\"\n",
    "NASA launched a new satellite into orbit. The satellite will observe solar storms. \n",
    "Elon Musk announced a new partnership with NASA. \n",
    "The mission is expected to last two years.\n",
    "\"\"\"\n",
    "\n",
    "# Process text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract named entities\n",
    "entities = {ent.text: ent.root for ent in doc.ents if ent.label_ in {\"PERSON\", \"ORG\", \"GPE\", \"NORP\"}}\n",
    "\n",
    "# Helper to build a short action phrase\n",
    "def get_action_phrase(verb_token):\n",
    "    phrase = [verb_token.text]\n",
    "    for child in verb_token.children:\n",
    "        if child.dep_ in {\"dobj\", \"pobj\", \"attr\", \"xcomp\", \"acomp\", \"nmod\"}:\n",
    "            subtree = ' '.join([t.text for t in child.subtree])\n",
    "            phrase.append(subtree)\n",
    "    return ' '.join(phrase)\n",
    "\n",
    "# Store results\n",
    "entity_actions = defaultdict(list)\n",
    "\n",
    "for sent in doc.sents:\n",
    "    for ent_text, ent_root in entities.items():\n",
    "        if ent_root in sent:\n",
    "            verb = None\n",
    "            role = None\n",
    "\n",
    "            if ent_root.dep_ in {\"nsubj\", \"nsubjpass\"}:\n",
    "                verb = ent_root.head\n",
    "                role = \"Subject\"\n",
    "            elif ent_root.dep_ in {\"dobj\", \"pobj\"}:\n",
    "                verb = ent_root.head\n",
    "                role = \"Object\"\n",
    "\n",
    "            if verb:\n",
    "                phrase = get_action_phrase(verb)\n",
    "                entity_actions[ent_text].append([role, phrase])\n",
    "\n",
    "# Pretty-print results\n",
    "for ent, actions in entity_actions.items():\n",
    "    print(f'\"{ent}\": {actions}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LoadData import LoadData\n",
    "from EntityDataset import DataLoader\n",
    "from TextFilter import TextFilter\n",
    "\n",
    "base_dir = \"train\"\n",
    "txt_file = \"subtask-1-annotations.txt\"\n",
    "subdirs = \"EN\"\n",
    "load_data = LoadData()\n",
    "data = load_data.load_data(base_dir, txt_file, subdirs)\n",
    "data_loader = DataLoader(data, base_dir)\n",
    "\n",
    "filter = TextFilter()\n",
    "\n",
    "for target, text in data_loader.yield_text():\n",
    "    print(target)\n",
    "    \n",
    "    print(filter.extract_target_context(text, target))\n",
    "    print()\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
